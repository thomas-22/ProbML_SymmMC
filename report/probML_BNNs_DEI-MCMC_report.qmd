---
title: |
  Bayesian Neural Networks:
  
  DEI-MCMC
  
  Symmetry & Mode Connectivity
subtitle: "[s25] BA-Seminar: Probabilistic ML (Bothmann)"
author: 
    - name: "Thomas Witzani\n
    
    Ludwig-Maximilians-Universität\nMünchen"
date: "Munich, 04 July 2025"
date-format: "4. Juli 2025"
lang: en

format:
  pdf:
    documentclass: scrreprt
    classoption: [12pt, onecolumn, open=any]
    bibliography: references.bib
    number-sections: true
    crossref:
      chapters: true
    keep_tex: true
    toc: true
    toc-depth: 2
    lof: false
    lot: false

    # Page geometry copied from the LMU template
    geometry:
      - a4paper
      - width=160mm
      - top=35mm
      - bottom=30mm
      - bindingoffset=0mm

    # Fonts / links
    mainfont: "Latin Modern Roman"
    colorlinks: true

    # Extra LaTeX injected in the preamble
    header-includes: |
      % keep figure / table numbers global
      \counterwithout{figure}{chapter}
      \counterwithout{table}{chapter}
      
      \usepackage{float}
      
      % remove the default ~50pt plus 20pt minus 20pt before every \chapter
      \renewcommand*{\chapterheadstartvskip}{\vspace*{0pt}}

      % running headline & footer (matches template style)
      \usepackage{fancyhdr}
      \pagestyle{fancy}
      \fancyhead{}
      \fancyhead[R]{\small\itshape Bayesian Neural Networks: DEI-MCMC}
      \fancyfoot{}
      \fancyfoot[R]{\thepage}
      \fancypagestyle{plain}{%
      \fancyhf{}                   
      \fancyfoot[C]{\thepage} 
      \renewcommand{\headrulewidth}{0pt}
      \renewcommand{\footrulewidth}{0pt}
      }
      
      \setlength{\headheight}{13.6pt}
      \linespread{1.05}
      \setkomafont{chapter}{\rmfamily\bfseries\LARGE}
      \setkomafont{chapterprefix}{\rmfamily\bfseries\LARGE}
      \setkomafont{section}{\rmfamily\bfseries\Large}
      \setkomafont{subsection}{\rmfamily\bfseries\large}
      \setkomafont{title}{\rmfamily\bfseries\Huge}
      \setkomafont{subtitle}{\rmfamily\bfseries\Large}
      \setkomafont{author}{\rmfamily\large}
      \setkomafont{date}{\rmfamily\large}
      \setkomafont{partentry}{\rmfamily}
      \setkomafont{chapterentry}{\rmfamily}
      
      \usepackage[
      backend=biber,
      style=numeric,
      sorting=nyt
      ]{biblatex}
      \addbibresource{references.bib}
      \renewcommand*{\chapterpagestyle}{fancy}
      
      \makeatletter
      \g@addto@macro\@author{\\[0.5em]{\small Ludwig-Maximilians-Universität München}}
      \makeatother
      
---


\chapter*{Abstract}
\addcontentsline{toc}{chapter}{Abstract}

Bayesian neural networks (BNNs) generalize standard neural networks (NNs) by placing probability distributions over weights (@neal1996bayesian & @bishop2006approximate) rather than relying on single point estimates. This enables principled quantification of predictive uncertainty (@gal2016dropout). In this work, I develop and evaluate a Deep-Ensemble-Initialized Markov Chain Monte Carlo (DEI-MCMC) workflow that trains a small ensemble of randomly-seeded networks, canonicalizes each network by neuron-sorting and sign-fixing, then clusters those canonicalized versions via cosine distance to select representatives on truly distinct posterior modes and uses those representatives to seed parallel "No U-Turn Sampler (NUTS)" (adaptive Hamiltonian Monte Carlo (HMC)) chains in Stan (@betancourt2017conceptual).

In a simulation study on a noisy sinusoidal function I tuned the workflow, tested methods and calibrated my approach. Finally, I apply DEI-MCMC to the UCI Airfoil Self-Noise dataset, demonstrating that symmetry-aware initialization selectively expands credible intervals in regions of genuine model disagreement while preserving narrow uncertainty elsewhere. Although the Airfoil posterior—in a parameter space modestly larger (~513 vs. ~449 dimensions)—yields more conservative mixing diagnostics, posterior predictive checks and feature-wise partial-dependence bands reveal that the sampler nonetheless captures meaningful uncertainty patterns. These findings confirm that canonicalized deep-ensemble seeding enables efficient exploration of challenging BNN posteriors and delivers robust, interpretable uncertainty estimates at a modest compute budget.

# Introduction & Motivation

## Motivation

A neural network (NN) is a parametric function
$$
\begin{aligned}
\quad
&f_{\theta}:\mathbb{R}^d\to\mathbb{R},\quad
\theta=\{W^{(1)},b^{(1)},\dots,W^{(L+1)},b^{(L+1)}\},\\
\end{aligned}
$$
defined layer-wise by
$$
\begin{aligned}
&h^{(0)}=x,\\
&h^{(\ell)}=\sigma\bigl(W^{(\ell)}\,h^{(\ell-1)}+b^{(\ell)}\bigr),
\quad \ell=1,\dots,L\\
&f_{\theta}(x)=W^{(L+1)}\,h^{(L)}+b^{(L+1)}
\end{aligned}
$$
where each $\sigma$ is a nonlinear activation. @hornik1989multilayer showed that for any compact set $K \subset \mathbb{R}^d$ and any $\varepsilon > 0$, there exists a single‐hidden‐layer network of sufficient width such that $$\sup_{x \in K}\bigl|f(x) - f_\theta(x)\bigr| < \varepsilon$$
i.e. NNs are universal approximators in the uniform norm.


In a BNN we treat $\theta$ as a random variable with a prior
$p(\theta)$ and observe the data
$$D = \{(x_i, y_i)\}_{i=1}^N$$ 
under the likelihood
$$p(D \mid \theta) = \prod_{i=1}^N p\bigl(y_i \mid f_\theta(x_i)\bigr).$$

Bayes' rule defines the posterior as
$$p(\theta \mid D) = \frac{p(D \mid \theta)\,p(\theta)}{\displaystyle\int p(D \mid \theta)\,p(\theta)\,d\theta}$$
which is a highly multimodal distribution in the $dim(\theta)$-dimensional parameter space.

The advantage of BNNs is that, instead of collapsing to a single point estimate $\hat{\theta}$, the network maintains a full posterior \mbox{$p(\theta\mid D)$}, which simultaneously quantifies the 
*aleatoric uncertainty* through the likelihood $p\bigl(y\mid f_{\theta}(x)\bigr)$, and 
*epistemic uncertainty* through the spread of the posterior itself (@hullermeier2019aleatoric,  @kendall2017uncertainties & @gelman2013single).

For a new input $x^*$ the BNNs posterior predictive distribution is
$$
p\bigl(y^* \mid x^*, D\bigr)
= \int p\bigl(y^* \mid f_\theta(x^*)\bigr)\,p\bigl(\theta \mid D\bigr)\,d\theta
$$
and the corresponding posterior-mean prediction (which is a point forecast) is
$$
\mathbb{E}\bigl[y^* \mid x^*, D\bigr]
= \int f_\theta(x^*)\,p\bigl(\theta \mid D\bigr)\,d\theta.
$$

## Challenges

The posterior predictive distribution admits a closed-form solution only under the restrictive, idealized assumption of conjugate priors and likelihoods (@gelman2013single). In practice, we almost always prefer richer priors and more realistic noise models, so we must fall back on approximate inference methods, namely MCMC (@betancourt2017conceptual).

Another challenge is the sheer dimensionality of the parameter space in a BNN. In a 5-16-16-16-8-1 architecture
the total number of trainable parameters is 513. In such high dimensions naïve MCMC samplers suffer from various 
problems such as vanishing acceptance rates, slow mixing and exponential cost because the volume of a high-dimensional space grows so fast that covering it uniformly is infeasible (@betancourt2017conceptual).

Moreover, BNN posteriors are inherently multimodal due to simple symmetries in the weight space. Two parameter settings ${\theta}$ and $\hat{\theta}$ are called equi-output if they define exactly the same input-output map,
$$
f_{\hat\theta}(x)=f_{\theta}(x)\quad \forall\,x
$$
even a tiny network exhibits many such symmetries.

The first type of symmetry arises through neuron permutations. In any hidden layer, the perceptrons are exchangeable: if you permute the columns of $W^{(l)}$ and simultaneously permute the rows of $W^{(l+1)}$, the 
overall function $f_{\theta}$ remains unchanged. The number of symmetries that arise through this mechanism is $$\prod_{\ell=1}^{L} n_\ell!.$$

The second type of symmetry comes from sign flips: whenever the activation $\sigma$ is odd (e.g. tanh), you can pick any hidden neuron in layer $\ell$, multiply its incoming weights and bias by -1, and at the same time multiply its outgoing weights by -1, without changing $f_{\theta}$. Since each of the hidden neurons can be flipped independently, there are $$2^{\sum_{\ell=1}^{L}n_\ell}$$ distinct sign-flip symmetries, creating a total of $$\prod_{\ell=1}^{L} n_\ell! × 2^{\sum_{\ell=1}^{L}n_\ell}$$ symmetrical modes.

If an MCMC sampler is unaware of these symmetries, it ends up wasting iterations on equi-output duplicates. This both inflates the apparent number of posterior modes and blows up parameter-space variance. Because these redundant draws add no new functional information, the effective sample size stalls, Monte Carlo error in credible-interval estimates grows, and the resulting intervals become too narrow—i.e. they underestimate the true aleatoric uncertainty.

By collapsing those symmetries up front (permuting each sample into a common reference ordering), you fold all redundant modes onto one. The sampler then explores only genuinely distinct modes, boosting functional effective sample size, cutting Monte Carlo error, and yielding credible intervals that reflect real functional variation rather than redundant copies.

## Objective

The goal of this paper is to develop and validate a Deep-Ensemble-Initialized Markov Chain Monte Carlo (DEI-MCMC) workflow for BNNs that achieves efficient posterior exploration and well-informed uncertainty estimates by removing trivial symmetries in weight space. Concretely, I aim to:

1. Train a small ensemble of M randomly-seeded feed-forward NNs, $\{\theta^{(m)}\}_{m=1}^M$.

2. Canonicalize each $\theta^{(m)}$ by neuron-sorting and sign-fixing, so that NNs belonging to the same symmetry group collapse to identical (or nearly identical) canonical forms.

3. Cluster the resulting canonical NNs (using cosine distance) and then select $K (\le M)$ representatives to ensure each final ensemble member lies on a functionally distinct posterior peak.

4. Initialize $K$ parallel NUTS chains with Stan starting at these $\{\tilde{\theta}^{(k)}\}_{k=1}^K$.

5. Assess convergence and evaluate uncertainty calibration via credible-interval coverage and posterior predictive checks.

# Related Work

## BNN Posterior Sampling Methods

@hoffman2011nuts introduces NUTS, an extension of HMC that discards the manually chosen trajectory length 
$L$. NUTS keeps doubling the leapfrog path until the simulated momentum would reverse toward the start—the “no-u-turn” stop. Together with primal–dual averaging for adaptive step size, this yields a self-tuning, gradient-based MCMC method that matches or beats well-tuned HMC without user calibration. Because manually selecting $L$ and step size in vanilla HMC is notoriously sensitive and labor-intensive, NUTS makes practical Bayesian inference vastly easier and more robust. 

This algorithm is now used by default in Stan, the R library for Bayesian modeling and inference that I used in this project.

## Deep-Ensemble Initialisation (DEI)

@sommer2024connecting show that a deep ensemble (a handful of independently initialised and fully trained neural networks) already lands its members in separate high-probability basins of the Bayesian posterior.  Starting HMC chains from those pre-optimised weights therefore eliminates much of the costly burn-in phase.  Complementary empirical evidence in @izmailov2021posteriors confirms that such ensemble seeds cover the dominant modes encountered by standard HMC, while chains started from random points often fail to reach them within a practical time & compute budget.  In this project I follow that recipe: train a small ensemble, take each member’s weights as an initial state, and launch parallel NUTS chains from those mode-finding seeds to achieve efficient convergence and broad coverage.


## Symmetry Detection and Elimination

@wiese2023towards demonstrate that permutation- and sign-flip symmetries create exponentially many equi-output modes that severely hamper MCMC efficiency. They introduce an inexpensive canonicalization map—sorting neurons layer-wise and fixing their signs—to collapse each symmetry class to a single representative while preserving the log-likelihood. After symmetry removal, @wiese2023towards further cluster the transformed samples in function space using a spectral clustering workflow. It builds a nearest-neighbour graph from the symmetry-removed samples, computes a normalized graph Laplacian embedding, and applies k-means to recover distinct modes. While spectral clustering can uncover arbitrarily shaped, overlapping clusters, my choice to flatten each canonical network into a single weight vector and cluster directly via cosine distance trades minimal geometric flexibility for far greater simplicity and speed—and in practice the post-canonicalization modes are sufficiently well-separated by angle to make cosine clustering both effective and scalable. 
I adopt this canonicalization as a preprocessing step so that subsequent NUTS chains explore only genuinely distinct regions of the BNN posterior.

## Mode Connectivity and Sample-Based Inference

Linearly interpolating between two SGD solutions usually produces a high-loss
ridge, but a series of works beginning with @garipov2018loss and refined by
@fort2019deep show that curved low-loss paths often exist, implying that
many apparent local optima belong to a larger, connected manifold.  Most
recently, @sommer2024connecting connect such paths directly to Bayesian inference:
they sample along the connector using tempering, obtaining predictive
distributions that rival full HMC at a fraction of the cost.  Although my
workflow focuses on isolated mode initialisation rather than traversing
connectors, these findings reinforce the idea that weight-space distance does
not automatically translate to functional diversity, motivating my additional
clustering step using cosine similarity in the canonical parameter space.

# Methods

## Data Preparation

### Simulation Study

To gather synthetic data I generate $n = 1500$ inputs $x_i \sim \mathcal{U}(-5,5)$ and define corresponding outputs by
$y_i = \sin(\pi x_i) + \epsilon_i,\quad \epsilon_i \sim \mathcal{N}(0,\,0.2^2)$.

### UCI Airfoil
Originally published via the UCI Machine Learning Repository, the Airfoil Self-Noise dataset comprises 1503 wind-tunnel measurements from NASA’s Langley Research Center. Each record captures the sound-pressure level of a standard airfoil under varying $Frequency$, $AngleAttack$, $ChordLength$, $Velocity$, and $SuctionThickness$. To improve numerical stability, I apply a $log(1+x)$ transform to $Frequency$ and then z-standardize all five predictors and the target $SoundPressure$. The computed means and standard deviations are saved so that Stan’s outputs can be converted back to the original decibel scale.

## Ensemble Training

In both workflows I train an ensemble of $M=4$ differently seeded feed-forward networks with architectures 1-16-16-8-1 for the synthetic data and 5-16-16-8-1 for the UCI Airfoil data. The NNs were trained in Keras using MSE loss, $tanh$ activation, the Adam optimizer and a batch size of 16. All models converged in less than 200 epochs.


## Canonicalization and clustering

Each NN is first mapped to a unique canonical form. Neurons are sorted by the $L_2$-norm of their outgoing weights, and any neuron whose leading outgoing weight is negative has all its incoming and outgoing weights (and bias) flipped, thereby collapsing all permutation and sign-flip symmetries. These canonical models are then flattened into P-dimensional weight vectors and clustered using cosine distance. In P dimensions, random unit vectors have expected cosine similarity zero (meaning an average angle of 90°, giving cosine distance 1), so any distance close to 0 would imply that the two NNs lay on the same mode.

## Stan (BNN) Setup

### Simulation Study

A 3 hidden-layer feed‐forward network on scalar inputs ($D=1$) is implemented via non‐centered parameters $z \sim \mathcal{N}(0,1)$, scaled by log-normal hyper‐priors $\sigma_W,\sigma_b$. Observation noise $\sigma$ likewise follows a log‐normal prior. The data likelihood is parallelized across data slices with Stan’s `reduce_sum` function—parallelizing over disjoint index slices—which dramatically speeds up NUTS sampling.

### UCI Airfoil

The same design extends to $D=5$ features by flattening the first weight matrix into a vector
$z_{W1\_flat}$. Remaining layers use non-centered $z$-matrices and vectors with shared log-normal scales and a single noise parameter $\sigma$. Again, Stan’s `reduce_sum` was used.

## MCMC Sampling

### Simulation Study
I launch $K = 4$ parallel NUTS chains in Stan, each initialized at the location at one of the 4 canonical NNs. After 250 warm-up steps, each chain collects 100 posterior samples, using adapt_delta = 0.95 and max_treedepth = 15 to balance exploration and computational cost. Preliminary two-chain runs confirmed these settings yield stable $\hat R$ diagnostics and sufficient effective sample sizes.

### UCI Airfoil
For the Airfoil Self-Noise problem I likewise initialize $K = 4$ NUTS chains at the four canonical ensemble modes. Each chain performs 350 warm-up steps and 125 sampling steps, with adapt_delta = 0.95 and max_treedepth = 18. These hyperparameters differ slightly, reflecting the increased complexity of this real-world dataset.

## Inspection & Evaluation
### Simulation Study

For the simulation study I compute $\hat R$ and $ESS$ (Effective Sample Size) across all 4 chains to evaluate chain samples. A 90% predictive credible band is constructed by overlaying MCMC draws (aligned via canonical permutations) on the true sine curve. This workflow both validates sampler performance against the known generative model and makes uncertainty visible in function space rather than weight space. Additionally, the 90% credible interval for the inferred noise parameter is compared against the theoretical 90% central interval of the true Gaussian noise ($\epsilon \sim \mathcal{N}(0,\,0.2^2)$), closing the loop on calibration.

### UCI Airfoil
For the UCI Airfoil dataset, the same convergence checks and posterior summaries for the noise term ensure reliable mixing across four chains. Predictive bands and partial‐dependence intervals are then plotted over each feature, combining ensemble seeds and DEI‐MCMC draws to map uncertainty across the real‐world response surface. By mirroring the synthetic workflow’s focus on mixing diagnostics and functional credible bands, this evaluation highlights where airfoil predictions carry the greatest uncertainty.

# DEI-MCMC Results  
## Simulation Study
### Ensemble & Canonicalization
```{r canon-distances-synth, echo=FALSE, results='asis', message=FALSE, warning=FALSE}
library(proxy)
library(knitr)
source("../R/03_canon.R")

# 1) Declare a placeholder
res_syn <- NULL

# 2) Run clustering, capturing all console output but still saving the return value
invisible(capture.output({
  res_syn <<- cluster_canonical_models(
    canon_paths = sprintf(
      "../results/ensemble_synth/sin_dataset_member%02d_canon.keras", 
      1:4
    ),
    threshold   = 0.1,
    metric      = "cosine",
    output_file = NULL
  )
}))

# 3) Now res_syn is the list you want
cd <- res_syn$centroid_distances
rownames(cd) <- colnames(cd) <- sprintf("NN%02d", seq_len(nrow(cd)))

# 4) Emit only the clean table
kable(
  cd,
  format   = "latex",
  booktabs = TRUE,
  digits   = 4,
  caption  = "Pairwise cosine distances between canonical ensemble members on the synthetic dataset"
)

```
![Ensemble Member predictions](../results/Figures/ensemble_result_synth.png){#fig-ensemble_synth_facet fig-cap="Ensemble Member predictions" fig-align="center" out-width="80%"}

Figure 1 shows the four ensemble members all recover the underlying $\sin(\pi x)$ function (red dashed) with high fidelity. Notably, the canonicalization step does not alter any predictions: the raw and canonicalized networks overlay perfectly.

Table 1 confirms that all 4 NNs lay in functionally distinct modes in the posterior parameter space.

\newpage

### MCMC (NUTS via Stan)
```{r mcmc-diagnostics-synth, echo=FALSE, results='asis', message=FALSE, warning=FALSE}
library(posterior)
library(knitr)
library(kableExtra)
source("../R/05_inspect_draws.R")

# ─── 1) Sampling settings table ───────────────────────────────────────────
settings_table <- data.frame(
  Parameter = c(
    "Number of chains",
    "Warmup iterations per chain",
    "Sampling iterations per chain",
    "Adapt delta",
    "Max Treedepth"
  ),
  Value = c(4, 250, 100, 0.95, 15),
  stringsAsFactors = FALSE
)

# ─── 2) Load each chain as a draws_array ─────────────────────────────────
chain_files <- sprintf("../results/mcmc_draws/sin_chain%02d_draws.rds", 1:4)
draws_arr_list <- lapply(chain_files, function(path) {
  as_draws_array(load_draws(path))
})

# ─── 3) Per-chain ESS_bulk (mean over parameters) ─────────────────────────
chain_stats <- lapply(draws_arr_list, summarize_draws)

chain_rows <- do.call(
  rbind,
  mapply(function(stats, i) {
    data.frame(
      Chain  = paste0("Chain ", i),
      Metric = "ESS_bulk",
      Value  = mean(stats$ess_bulk, na.rm = TRUE),
      stringsAsFactors = FALSE
    )
  },
  stats = chain_stats,
  i = seq_along(chain_stats),
  SIMPLIFY = FALSE
  )
)

# ─── 4) Overall diagnostics across all chains ──────────────────────────────
da_all <- bind_draws(draws_arr_list, along = "chain")
st_all <- summarize_draws(da_all)
r_all  <- st_all$rhat
e_all  <- st_all$ess_bulk

overall_rows <- data.frame(
  Chain  = "All chains",
  Metric = c("R̂", "ESS_bulk"),
  Value  = c(mean(r_all), mean(e_all)),
  stringsAsFactors = FALSE
)

diag_table <- rbind(chain_rows, overall_rows)

# ─── 5) Emit side-by-side with minipages ─────────────────────────────────
cat("\\begin{minipage}[t]{0.48\\textwidth}\n")
print(
  kable(settings_table, "latex", booktabs = TRUE,
        caption = "MCMC Settings: Synthetic") %>%
    kable_styling(latex_options = "HOLD_position", full_width = FALSE)
)
cat("\\end{minipage}\\hfill\n")

cat("\\begin{minipage}[t]{0.48\\textwidth}\n")
print(
  kable(diag_table, "latex", booktabs = TRUE, digits = 3,
        caption = "Convergence Diagnostics:\\\\ Synthetic") %>%
    kable_styling(latex_options = "HOLD_position", full_width = FALSE)
)
cat("\\end{minipage}\n")
```


\begin{figure}[H]
  \centering
  \includegraphics[width=0.70\textwidth]{../results/Figures/90cb_pm_synth_4c.png}
  \caption{90\% Credibility Interval overlayed over true 90\% noise Interval}
  \label{fig:90cb_pm_synth_facet}
\end{figure}

\newpage
## UCI Airfoil dataset
### Ensemble & Canonicalization

\begin{figure}[H]
  \centering
  \includegraphics[width=1\textwidth]{../results/Figures/uci_pd_comp_4nn.png}
  \caption{Partial Dependence comparison between all 4 canonicalized NNs}
  \label{fig:fig-uci_ensemble_facet}
\end{figure}

Figure 3 presents the partial‐dependence curves for each of the four canonical ensemble members across all input features. Each network produces visibly different response patterns and when viewed alongside the high pairwise cosine distances in Table 4, this confirms that the models occupy functionally distinct modes in the BNN posterior.

```{r canon-distances-airfoil, echo=FALSE, results='asis', message=FALSE, warning=FALSE}
library(proxy)
library(knitr)
source("../R/03_canon.R")

# 1) Declare a placeholder
res_syn <- NULL

# 2) Run clustering, capturing all console output but still saving the return value
invisible(capture.output({
  res_syn <<- cluster_canonical_models(
    canon_paths = sprintf(
      "../results/ensemble_airfoil/airfoil_member%02d_canon.keras", 
      1:4
    ),
    threshold   = 0.1,
    metric      = "cosine",
    output_file = NULL
  )
}))

# 3) Now res_syn is the list you want
cd <- res_syn$centroid_distances
rownames(cd) <- colnames(cd) <- sprintf("NN%02d", seq_len(nrow(cd)))

library(kableExtra)

# 4) Emit only the clean table
kable(
  cd,
  format   = "latex",
  booktabs = TRUE,
  digits   = 4,
  caption  = "Pairwise cosine distances between canonical ensemble members on the UCI Airfoil dataset"
) %>%
  kable_styling(latex_options = "HOLD_position")

```

\newpage
### MCMC (NUTS via Stan)

```{r mcmc-diagnostics-airfoil, echo=FALSE, results='asis', message=FALSE, warning=FALSE}
library(posterior)
library(knitr)
library(kableExtra)
source("../R/05_inspect_draws.R")

# ─── 1) Sampling settings table ───────────────────────────────────────────
settings_table <- data.frame(
  Parameter = c(
    "Number of chains",
    "Warmup iterations per chain",
    "Sampling iterations per chain",
    "Adapt delta",
    "Max Treedepth"
  ),
  Value = c(4, 350, 125, 0.95, 18),
  stringsAsFactors = FALSE
)

# ─── 2) Load each chain as a draws_array ─────────────────────────────────
chain_files <- sprintf(
  "../results/mcmc_airfoil/airfoil_member%02d_canon_MCMC_draws.rds",
  1:4
)
draws_arr_list <- lapply(chain_files, function(path) {
  as_draws_array(load_draws(path))
})

# ─── 3) Per-chain ESS_bulk (mean over parameters) ─────────────────────────
chain_stats <- lapply(draws_arr_list, summarize_draws)

chain_rows <- do.call(
  rbind,
  mapply(function(stats, i) {
    data.frame(
      Chain  = paste0("Chain ", i),
      Metric = "ESS_bulk",
      Value  = mean(stats$ess_bulk, na.rm = TRUE),
      stringsAsFactors = FALSE
    )
  },
  stats = chain_stats,
  i = seq_along(chain_stats),
  SIMPLIFY = FALSE
  )
)

# ─── 4) Overall diagnostics across all chains ──────────────────────────────
da_all <- bind_draws(draws_arr_list, along = "chain")
st_all <- summarize_draws(da_all)
r_all  <- st_all$rhat
e_all  <- st_all$ess_bulk

overall_rows <- data.frame(
  Chain  = "All chains",
  Metric = c("R̂", "ESS_bulk"),
  Value  = c(mean(r_all), mean(e_all)),
  stringsAsFactors = FALSE
)

diag_table <- rbind(chain_rows, overall_rows)

# ─── 5) Emit side-by-side with minipages ─────────────────────────────────
cat("\\begin{minipage}[t]{0.48\\textwidth}\n")
print(
  kable(settings_table, "latex", booktabs = TRUE,
        caption = "MCMC Settings: UCI Airfoil") %>%
    kable_styling(latex_options = "HOLD_position", full_width = FALSE)
)
cat("\\end{minipage}\\hfill\n")

cat("\\begin{minipage}[t]{0.48\\textwidth}\n")
print(
  kable(diag_table, "latex", booktabs = TRUE, digits = 3,
        caption = "Convergence Diagnostics:\\\\ UCI Airfoil") %>%
    kable_styling(latex_options = "HOLD_position", full_width = FALSE)
)
cat("\\end{minipage}\n")
```


\begin{figure}[H]
  \centering
  \includegraphics[width=1\textwidth]{../results/Figures/airfoil_pd_dei_01.png}
  \caption{Partial dependence for \texttt{airfoil\_member01\_canon}: posterior samples and 90\,\% credible band}
  \label{fig:uci-pd-dei-01}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=1\textwidth]{../results/Figures/airfoil_pd_dei_02.png}
  \caption{Partial dependence for \texttt{airfoil\_member02\_canon}: posterior samples and 90\,\% credible band}
  \label{fig:uci-pd-dei-02}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=1\textwidth]{../results/Figures/airfoil_pd_dei_03.png}
  \caption{Partial dependence for \texttt{airfoil\_member03\_canon}: posterior samples and 90\,\% credible band}
  \label{fig:uci-pd-dei-03}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=1\textwidth]{../results/Figures/airfoil_pd_dei_04.png}
  \caption{Partial dependence for \texttt{airfoil\_member04\_canon}: posterior samples and 90\,\% credible band}
  \label{fig:uci-pd-dei-04}
\end{figure}





# Discussion
## Simulation Study
On the synthetic dataset, the four‐member ensemble captures the target function very closely. The NNs struggle merely at the edges due to sparse data. After canonicalization the individual forecasts in Figure 1 remain identical, while the pair-wise cosine distances in Table 1 verify that each seed sits on a genuinely different posterior peak. 

Seeding NUTS with those diverse yet symmetry-free parameters pays off: all chains achieve respectable bulk-$ESS$ values (between ~ 75 and ~ 124) and a $\hat R$ of 1.05 (Table 3), indicating healthy mixing around their respective modes. 

As a result, the aggregated posterior predictive in Figure 2 hugs the true $sin(\pi x)$ curve, yet the 90 % credibility band is much tighter than the interval that contains 90% of the true noise $\epsilon_i \sim \mathcal{N}(0,\,0.2^2)$. This means that the aggregate chains’ estimate of the true uncertainty is too low, implying an overconfident predictive posterior. This is likely because the true posterior has more functionally different modes than 4.



## UCI Airfoil dataset
On the UCI Airfoil data, the result is more nuanced. The initial ensemble again lands on genuinely different functional modes: the contrasting partial-dependence curves in Figure 3 and the large pairwise distances in Table 4 confirm this diversity, demonstrating that the canonicalization & clustering step is working as intended.

However, sampling from the more complex posterior yields a bulk $ESS$ of less than ~41 per chain and a pooled $\hat R$ of 2.10 (Table 6), indicating that each chain did not fully explore its initial weight-space basin within the allotted compute budget—approximately 12 h on an AMD Ryzen 5 5600X at 100 % utilization (see Table 5).

Nevertheless, the NUTS draws broaden the narrow deterministic curves of the ensemble into predictive bands (see Figures 4–7), showing uncertainty precisely where the original networks disagree most (e.g. in the feature *SuctionThickness* between 0.03 and ~ 0.06). 

In sum, the air-foil experiment mainly exposes my computational budget, not a fundamental weakness of DEI or MCMC (NUTS): with more time or higher-performance hardware, NUTS should still explore the tough real-world posterior effectively. Even so, the symmetry-aware initialization already yields richer and more interpretable uncertainty than the raw ensemble on its own.




# Conclusion
This study set out to test whether a Deep-Ensemble-Initialised Markov Chain Monte Carlo (DEI-MCMC) workflow—including symmetry removal applied after ensembling—can turn fully Bayesian neural-network inference from a theoretical ideal into something that runs overnight on standard consumer hardware. By first training a handful of neural networks, collapsing permutation- and sign-flip symmetries via canonicalization, clustering the resulting weight vectors and then launching one NUTS chain per cluster, I deliberately separated the mode-finding and mode-exploring phases of inference. The promise of efficient posterior exploration without surrendering accuracy has largely been demonstrated in practice.

In the simulation study, the four-member ensemble already landed on functionally distinct posterior peaks; canonicalization and clustering then preserved those genuine differences by collapsing only the symmetry-induced duplicates. In practice, the networks had largely converged to distinct modes on their own. Seeding NUTS at those locations produced healthy bulk-$ESS$ values between ~75 and ~124 and a $\hat R$ of 1.05 (Table 3), indicating good within-mode mixing, yet the aggregated 90 % credibility band remained too tight, evidence that four chains undersampled the true number of modes, leading to an underestimation of uncertainty.

On the UCI Airfoil Self-Noise data the same approach broadened the ensemble’s overly confident point forecasts into sensible posterior bands: uncertainty widened precisely where the raw networks disagreed most. At the same time the tougher, higher-dimensional posterior exposed a limitation of the current implementation: with only 125 sampling steps per chain (Table 5) the bulk-$ESS$ dropped below 41 and the pooled $\hat R$ rose to 2.10, signalling relatively poor exploration within each basin (Table 6).

Across both cases the symmetry-aware initialisation proved its worth: it removed redundant traversals of equivalent parameter states, in theory, and gave every Monte-Carlo step a chance to learn something new about the functional landscape of the network. Where the workflow falls short, the bottlenecks are computational rather than conceptual: deeper data sets and richer priors simply demand either more chains, more samples, or faster hardware.

In sum, DEI-MCMC offers a principled, reproducible route to calibrated Bayesian predictions in moderately sized neural networks. Its present incarnation already outperforms a plain deep ensemble in uncertainty quality while adding only moderate overhead; its limitations are understood and, as the next section argues, eminently addressable.

# Outlook
Several avenues could extend this work from a proof-of-concept into a robust toolbox for large-scale Bayesian deep learning.

First, larger and more diverse ensembles should reduce the risk of missing functionally unique modes. A systematic sweep that grows the ensemble size until additional members cease to enlarge the posterior support—measured, for example, by the clustering distances already used here—would quantify the diminishing returns of more seeds and help formalise the trade-off between number of chains and compute time.

Second, recent findings on mode connectivity suggest that many apparent optima are merely well-separated points on a low-loss manifold (@garipov2018loss; @fort2019deep). Bridging canonicalized modes with curved “connectors” and letting HMC sample along those paths, rather than inside isolated modes, could merge formerly disconnected weight-space islands into a single, navigable region—potentially reducing the number of required chains even further.

Third, the workflow now runs in Stan on a single CPU. Porting the model to GPU-enabled frameworks such as NumPyro, PyTorch + Pyro or TensorFlow Probability would make gradient evaluations orders of magnitude faster and unlock bigger architectures. Until Stan gains first-class CUDA support, a lightweight re-implementation of the canonicalization map in JAX followed by NumPyro’s GPU-native NUTS seems like the most direct path.

Fourth, canonicalization is currently applied once, before sampling. Embedding a dynamic lightweight symmetry-resolution step into each leap-frog update could keep chains from drifting back into redundant regions and might improve mixing in the later layers, where over-parameterisation smooths the landscape but also multiplies symmetries.

Fifth, richer diagnostics tailored to multimodal, high-variance posteriors—such as the chain- and layer-wise $\hat R$ measure introduced by @wiese2023towards should accompany any scaling effort to ensure that increased throughput translates into genuinely better uncertainty estimates rather than a cascade of poorly mixed samples.

Sixth, a growing body of work has investigated how many distinct modes are truly needed to approximate a complex posterior. @tiulpin2021greedy introduce a greedy, submodular selection scheme that sequentially adds ensemble members until the marginal gain in divergence falls below a threshold.

Empirical studies have also shown that ensembles of only five to eight networks suffice to capture almost all of the gains from Bayesian marginalization in deep models (@ovadia2019can & @lakshminarayanan2016simple).

Likewise, @lakshminarayanan2016simple show empirically that beyond three to five independent restarts, further ensemble members yield negligible improvement in uncertainty calibration or predictive performance. Incorporating such mode-stopping criteria into DEI-MCMC—stopping once the ensemble covers a pre-specified fraction of the posterior probability mass—could dramatically trim computational cost without sacrificing inferential quality.

Pursuing these threads—bigger ensembles, connector-aware sampling, GPU acceleration, dynamic symmetry resolution, richer diagnostics, and principled mode selection—promises to move Bayesian neural-network inference from the realm of careful case studies toward a routine option for real-world modelling.

# Appendix

GitHub repo for this work: [thomas-22/ProbML_SymmMC](https://github.com/thomas-22/ProbML_SymmMC)

\begin{figure}[H]
  \centering
  \includegraphics[width=1\textwidth]{../results/Figures/synth_pp_sample_mean_4c.png}
  \caption{Posterior predictive sine function.}
  \label{fig:pp_synth_facet}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=1\textwidth]{../results/Figures/traceplot_sigma_synthetic.png}
  \caption{Traceplot of sigma for the synthetic dataset.}
  \label{fig:trace_synthetic}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=1\textwidth]{../results/Figures/traceplot_sigma_w1_airfoil.png}
  \caption{Traceplot of sigma and $W_{1}[1,1]$ for the UCI Airfoil dataset.}
  \label{fig:trace_airfoil}
\end{figure}

# References
::: {#refs}
:::


\chapter*{Acknowledgements}
\addcontentsline{toc}{chapter}{Acknowledgements}
I would like to express my deep gratitude to Prof.\ Dr.\ Bothmann for his invaluable guidance and support throughout the development of this work.

My thanks also go to Lisa Wimmer for the enlightening lecture on uncertainty delivered during our seminar.

Finally, I thank my peers in the Probabilistic ML seminar for the thoughtful conversations and guidance, which helped me refine my approach.

\chapter*{Declaration of authorship}
\addcontentsline{toc}{chapter}{Declaration of authorship}

I hereby declare that the report submitted is my own unaided work. All direct 
or indirect sources used are acknowledged as references. I am aware that the 
Thesis in digital form can be examined for the use of unauthorized aid and in 
order to determine whether the report as a whole or parts incorporated in it may 
be deemed as plagiarism. For the comparison of my work with existing sources I 
agree that it shall be entered in a database where it shall also remain after 
examination, to enable comparison with future Theses submitted. Further rights 
of reproduction and usage, however, are not granted here. This paper was not 
previously presented to another examination board and has not been published.

\vspace*{\fill}
\noindent Munich, 4. Juli 2025

\begin{flushleft}
  \includegraphics[width=4cm,keepaspectratio]{signature.png}
\end{flushleft}



\chapter*{Declaration of AI use}
\addcontentsline{toc}{chapter}{Declaration of AI use}

The author acknowledges the use of AI tools throughout the preparation of work. In particular, ChatGPT o3 and ChatGPT o4-mini-high were employed for brainstorming, gaining a general understanding of the subject matter, identifying relevant sources and generating ideas for further steps. 

Any code that wasn't fully manually written was aided by o4mini-high almost exclusively; however, every code snippet produced by the model was manually reviewed, tested and most often manually changed to ensure clarity and correctness. 

Claude Sonnet 4 was employed selectively to assist with debugging the Stan model. ChatGPT o4mini-high also supported the author’s comprehension of the Keras & Stan library.

All text in this report was initially drafted by hand, then partly rephrased by ChatGPT o3 to improve readability and flow and finally manually edited to ensure accuracy and style consistency.

All information was crosschecked manually with relevant verified sources.

No AI‐generated content was used without rigorous oversight and revision through the author.  

